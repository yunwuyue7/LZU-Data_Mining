{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc9de9b-f480-4fe6-84e4-50fb752c7773",
   "metadata": {},
   "source": [
    "# Data Mining Project4: Cluster on Given Dataset \n",
    "- e-mail: niejy20@lzu.edu.cn\n",
    "- data：June 13th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035bab1-b6fa-429b-b5d4-ea5e15287937",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f5413-1e64-4376-ae1a-34f586b26c69",
   "metadata": {},
   "source": [
    "## 1.1 数据集简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ec7a2-499a-45c8-8128-10e915f6bf8d",
   "metadata": {},
   "source": [
    "### GSE235508 转录组数据集\n",
    "\n",
    "- **数据来源**：来自类风湿关节炎（RA）、系统性红斑狼疮（SLE）患者及健康孕妇的血液转录组数据，旨在分析妊娠期免疫调节的基因表达差异。\n",
    "- **分类任务**：将样本分为不同组别（如 `HEALTHY`、`SPRA`、`SLE`），属于多分类问题（具体类别需根据 `samplegroup:ch1` 的取值确定）。\n",
    "- **数据规模**：包含 **335 个样本**，每个样本有 **60,218 个基因表达特征**（CPM 值），属于典型的高维小样本数据。\n",
    "- **特征特点**：特征为基因表达量，需进行标准化处理（如 log 转换），且存在大量零值或低方差基因，需进行特征筛选。\n",
    "\n",
    "---\n",
    "\n",
    "### 数据集对比\n",
    "- 在第一次作业中，我使用了数据挖掘领域较为经典的数据集：Breast Cancer Wisconsin，与本次作业的数据集同样应用于医学领域，这两个数据集的区别如下：\n",
    "\n",
    "| 特征                  | GSE235508 转录组数据集       | Breast Cancer Wisconsin 数据集 |\n",
    "|-----------------------|------------------------------|---------------------------------|\n",
    "| **数据量**            | 335 个样本                   | 569 个样本                      |\n",
    "| **特征数量**          | 60,218 个基因表达特征        | 30 个形态学特征                 |\n",
    "| **应用领域**          | 自身免疫疾病研究             | 乳腺癌医学诊断                  |\n",
    "| **分类任务**          | 多分类（疾病状态分组）       | 二分类（良性 vs 恶性）          \n",
    "| **数据挑战**          | 高维度、小样本、特征稀疏     | 小样本、特征可解释性高          |\n",
    "| **典型预处理方法**    | 标准化、特征选择、降维       | 标准化、特征相关性分析          |\n",
    "\n",
    "---\n",
    "\n",
    "### 对比分析\n",
    "\n",
    "1. **数据维度差异**  \n",
    "   - GSE235508 的特征数量（60k+）远超 Breast Cancer（30），需采用策略避免维度灾难（**PCA、t-SNE 或 LASSO 特征选择**等） 。\n",
    "   - 与 Breast Cancer 数据集相比，GSE235508 的样本量更小，但特征维度更高，容易导致模型过拟合。\n",
    "\n",
    "2. **领域特异性**  \n",
    "   - **医学转录组数据** 的基因表达特征具有生物学意义，但需结合通路分析（如 GSEA）增强可解释性。\n",
    "   - 不同于 MiniBooNE 的物理信号，基因表达数据通常需 **log 转换** 和 **批次效应校正**。\n",
    "\n",
    "---\n",
    "\n",
    "通过对比可见，GSE235508 的 **高维小样本特性** 为数据挖掘带来了挑战。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2298071-066f-4f83-afa7-e57d9e7e5236",
   "metadata": {},
   "source": [
    "## 1.2 聚类算法简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f1858-aaed-4c6a-a498-c713a6663fb2",
   "metadata": {},
   "source": [
    "这是1.2的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c0f68-4c50-452d-b99e-576c78448286",
   "metadata": {},
   "source": [
    "# 2. Cluster (10 algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc668cb-6912-44dc-a8f4-b847a469e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering, Birch\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import hdbscan\n",
    "import warnings\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8757bf40-4ef7-49d0-935d-bcf2f37919b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Enhanced Data Loader\n",
    "class ClusteringAnalyzer:\n",
    "    def __init__(self, expr_path, meta_path):\n",
    "        self.expr_path = expr_path\n",
    "        self.meta_path = meta_path\n",
    "        self.X = None\n",
    "        self.meta = None\n",
    "        self.labels_group = None\n",
    "        self.labels_das28 = None\n",
    "        \n",
    "    def load_and_preprocess_data(self, log_transform=True, var_threshold=0.1, pca_components=0.95):\n",
    "        # Load expression data\n",
    "        expr = pd.read_csv(self.expr_path, sep='\\t', comment='!', index_col=0, encoding='utf-8').T\n",
    "        \n",
    "        # Load metadata\n",
    "        meta = pd.read_csv(self.meta_path, sep='\\t', quotechar='\"', dtype=str)\n",
    "        meta = meta[['geo_accession', 'samplegroup:ch1', 'das28:ch1']]\n",
    "        meta.columns = ['sample_id', 'group', 'das28']\n",
    "        \n",
    "        # Merge data\n",
    "        expr.index.name = 'sample_id'\n",
    "        merged = expr.merge(meta, left_index=True, right_on='sample_id').set_index('sample_id')\n",
    "        \n",
    "        # Handle missing values\n",
    "        merged['das28'] = pd.to_numeric(merged['das28'].replace('NA', np.nan), errors='coerce').fillna(0)\n",
    "        \n",
    "        # Create target labels\n",
    "        le_group = LabelEncoder()\n",
    "        self.labels_group = le_group.fit_transform(merged['group'])\n",
    "        \n",
    "        conditions = [\n",
    "            merged['das28'] < 1,\n",
    "            (merged['das28'] >= 1) & (merged['das28'] < 3),\n",
    "            merged['das28'] >= 3\n",
    "        ]\n",
    "        choices = [0, 1, 2]  # Low, Medium, High\n",
    "        self.labels_das28 = np.select(conditions, choices, default=0)\n",
    "        \n",
    "        # Feature matrix\n",
    "        X = merged.drop(['group', 'das28'], axis=1).astype(float)\n",
    "        \n",
    "        # Log transformation\n",
    "        if log_transform:\n",
    "            X = np.log1p(X)\n",
    "            \n",
    "        # Filter low variance genes\n",
    "        selector = VarianceThreshold(threshold=var_threshold)\n",
    "        X = selector.fit_transform(X)\n",
    "        \n",
    "        # Standardize data\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "        \n",
    "        # Dimensionality reduction\n",
    "        pca = PCA(n_components=pca_components, random_state=42)\n",
    "        self.X = pca.fit_transform(X)\n",
    "        \n",
    "        print(f\"Data preprocessing complete. Final dimensions: {self.X.shape}\")\n",
    "        print(f\"PCA explained variance: {np.sum(pca.explained_variance_ratio_):.2f}\")\n",
    "        \n",
    "        return self.X, self.labels_group, self.labels_das28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81b5d97-0ed9-4663-abcb-f1eab360345d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa1 in position 169863: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m meta_path = \u001b[33m\"\u001b[39m\u001b[33m./Data/GSE235508.meta.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m analyzer = ClusteringAnalyzer(expr_path, meta_path)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X, labels_group, labels_das28 = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvar_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased variance threshold\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpca_components\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Retain more variance\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mClusteringAnalyzer.load_and_preprocess_data\u001b[39m\u001b[34m(self, log_transform, var_threshold, pca_components)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_preprocess_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_transform=\u001b[38;5;28;01mTrue\u001b[39;00m, var_threshold=\u001b[32m0.1\u001b[39m, pca_components=\u001b[32m0.95\u001b[39m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Load expression data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     expr = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexpr_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m!\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.T\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Load metadata\u001b[39;00m\n\u001b[32m     16\u001b[39m     meta = pd.read_csv(\u001b[38;5;28mself\u001b[39m.meta_path, sep=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m, quotechar=\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m, dtype=\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xa1 in position 169863: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Loading\n",
    "# Update paths according to your environment\n",
    "expr_path = \"./Data/GSE235508_mRNA_counts.txt\"\n",
    "meta_path = \"./Data/GSE235508.meta.txt\"\n",
    "\n",
    "analyzer = ClusteringAnalyzer(expr_path, meta_path)\n",
    "X, labels_group, labels_das28 = analyzer.load_and_preprocess_data(\n",
    "    log_transform=True,\n",
    "    var_threshold=0.5,  # Increased variance threshold\n",
    "    pca_components=0.99  # Retain more variance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25380d2-af00-46f7-93e8-a82a45df7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Clustering Evaluation Helper\n",
    "def evaluate_clustering(X, labels_true, algorithm, params):\n",
    "    \"\"\"\n",
    "    Evaluate clustering algorithm with given parameters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = algorithm(**params)\n",
    "        \n",
    "        if hasattr(model, 'fit_predict'):\n",
    "            clusters = model.fit_predict(X)\n",
    "        else:\n",
    "            model.fit(X)\n",
    "            clusters = model.labels_\n",
    "            \n",
    "        # Handle noise points in DBSCAN/HDBSCAN\n",
    "        if -1 in clusters:\n",
    "            clusters[clusters == -1] = max(clusters) + 1\n",
    "            \n",
    "        ari = adjusted_rand_score(labels_true, clusters)\n",
    "        sil_score = silhouette_score(X, clusters) if len(np.unique(clusters)) > 1 else -1\n",
    "        \n",
    "        return {\n",
    "            'params': params,\n",
    "            'clusters': clusters,\n",
    "            'ari': ari,\n",
    "            'silhouette': sil_score,\n",
    "            'n_clusters': len(np.unique(clusters))\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {algorithm.__name__} and params {params}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2105c-a45b-4f0b-9c0c-ad4afab07e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: KMeans Clustering with Tuning\n",
    "kmeans_results = []\n",
    "param_grid = {\n",
    "    'n_clusters': [2, 3, 4, 5],\n",
    "    'init': ['k-means++', 'random'],\n",
    "    'n_init': [10, 20],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    result = evaluate_clustering(X, labels_group, KMeans, params)\n",
    "    if result:\n",
    "        kmeans_results.append(result)\n",
    "\n",
    "# Find best result\n",
    "best_kmeans = max(kmeans_results, key=lambda x: x['ari'])\n",
    "print(f\"Best KMeans - ARI: {best_kmeans['ari']:.4f}, Silhouette: {best_kmeans['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b65f99-62bd-4b6f-b317-24e54b4bc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Hierarchical Clustering with Tuning\n",
    "hierarchical_results = []\n",
    "param_grid = {\n",
    "    'n_clusters': [2, 3, 4, 5],\n",
    "    'linkage': ['ward', 'complete', 'average'],\n",
    "    'affinity': ['euclidean', 'cosine']\n",
    "}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    result = evaluate_clustering(X, labels_group, AgglomerativeClustering, params)\n",
    "    if result:\n",
    "        hierarchical_results.append(result)\n",
    "\n",
    "best_hierarchical = max(hierarchical_results, key=lambda x: x['ari'])\n",
    "print(f\"Best Hierarchical - ARI: {best_hierarchical['ari']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00a4fd-716d-4ed3-98e6-4b24f4d37efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: DBSCAN Clustering with Tuning\n",
    "dbscan_results = []\n",
    "param_grid = {\n",
    "    'eps': [0.3, 0.5, 0.7, 1.0, 1.5],\n",
    "    'min_samples': [3, 5, 10]\n",
    "}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    result = evaluate_clustering(X, labels_group, DBSCAN, params)\n",
    "    if result:\n",
    "        dbscan_results.append(result)\n",
    "\n",
    "if dbscan_results:\n",
    "    best_dbscan = max(dbscan_results, key=lambda x: x['ari'])\n",
    "    print(f\"Best DBSCAN - ARI: {best_dbscan['ari']:.4f}\")\n",
    "else:\n",
    "    print(\"No valid DBSCAN parameters found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb2299-409b-41c3-af03-5c388411e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Gaussian Mixture Model with Tuning\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm_results = []\n",
    "param_grid = {\n",
    "    'n_components': [2, 3, 4, 5],\n",
    "    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    try:\n",
    "        model = GaussianMixture(**params)\n",
    "        model.fit(X)\n",
    "        clusters = model.predict(X)\n",
    "        \n",
    "        ari = adjusted_rand_score(labels_group, clusters)\n",
    "        sil_score = silhouette_score(X, clusters)\n",
    "        \n",
    "        gmm_results.append({\n",
    "            'params': params,\n",
    "            'clusters': clusters,\n",
    "            'ari': ari,\n",
    "            'silhouette': sil_score,\n",
    "            'n_clusters': params['n_components']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error with GMM and params {params}: {str(e)}\")\n",
    "\n",
    "best_gmm = max(gmm_results, key=lambda x: x['ari'])\n",
    "print(f\"Best GMM - ARI: {best_gmm['ari']:.4f}, Silhouette: {best_gmm['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93c1c0-2f2d-44ef-9f04-aa6f0804a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization of Best Results\n",
    "def visualize_results(X, true_labels, cluster_labels, algorithm_name):\n",
    "    \"\"\"Visualize clustering results using t-SNE\"\"\"\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=true_labels, palette='viridis', s=50, alpha=0.8)\n",
    "    plt.title(f'True Groups ({algorithm_name})', fontsize=14)\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=cluster_labels, palette='viridis', s=50, alpha=0.8)\n",
    "    plt.title(f'{algorithm_name} Clustering Results', fontsize=14)\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{algorithm_name}_clustering.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize best algorithms\n",
    "visualize_results(X, labels_group, best_kmeans['clusters'], 'KMeans')\n",
    "visualize_results(X, labels_group, best_hierarchical['clusters'], 'Hierarchical')\n",
    "visualize_results(X, labels_group, best_gmm['clusters'], 'GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda30766-f90d-4b4b-8cb6-e37c137aba8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da7b64-3993-45c5-8906-08718d1491be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9efef3e-eb5e-48c0-9b78-65cbda065261",
   "metadata": {},
   "source": [
    "# 3. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b683a2-dd97-4eaa-a93c-6d664177c15e",
   "metadata": {},
   "source": [
    "## 3.1 聚类模型效果对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41492a58-16cc-4bcd-af59-9b57aceb4dcd",
   "metadata": {},
   "source": [
    "## 3.2 作业总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3280d67-567b-494d-a8ff-3365e8da39b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
